{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from eval import accuracy\n",
    "from model import MnistCNN\n",
    "from train import fit, evaluate\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "NUM_CLASSES = 10\n",
    "INPUT_SIZE = 28 * 28\n",
    "DATASET = MNIST(root='data/', download=True, train=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\" \n",
    "\n",
    "def split_indices(n, val_pct):\n",
    "    # Determine size of validation set\n",
    "    n_val = int(val_pct*n)\n",
    "\n",
    "    idxs = np.random.permutation(n)\n",
    "\n",
    "    return idxs[n_val:], idxs[:n_val]\n",
    "\n",
    "train_indexes, validation_indexes = split_indices(len(DATASET), 0.2)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indexes)\n",
    "train_loader = DataLoader(DATASET, BATCH_SIZE, sampler=train_sampler)\n",
    "\n",
    "val_sampler = SubsetRandomSampler(validation_indexes)\n",
    "val_loader = DataLoader(DATASET, BATCH_SIZE, sampler=val_sampler)\n",
    "\n",
    "loss_fun = F.cross_entropy\n",
    "\n",
    "def predict_image(image, model):\n",
    "    xb = image.unsqueeze(0).to(dev)\n",
    "    yb = model(xb)\n",
    "    yb = yb.to(dev)\n",
    "    _, preds = torch.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.3038, accuracy: 0.9106\n",
      "Epoch 2, loss: 0.1961, accuracy: 0.9415\n",
      "Epoch 3, loss: 0.1523, accuracy: 0.9550\n",
      "Epoch 4, loss: 0.1301, accuracy: 0.9603\n",
      "Epoch 5, loss: 0.1175, accuracy: 0.9638\n",
      "Epoch 6, loss: 0.1084, accuracy: 0.9662\n",
      "Epoch 7, loss: 0.0971, accuracy: 0.9708\n",
      "Epoch 8, loss: 0.0940, accuracy: 0.9704\n",
      "Epoch 9, loss: 0.0879, accuracy: 0.9734\n",
      "Epoch 10, loss: 0.0866, accuracy: 0.9746\n",
      "Epoch 11, loss: 0.0795, accuracy: 0.9764\n",
      "Epoch 12, loss: 0.0770, accuracy: 0.9771\n",
      "Epoch 13, loss: 0.0756, accuracy: 0.9770\n",
      "Epoch 14, loss: 0.0753, accuracy: 0.9773\n",
      "Epoch 15, loss: 0.0756, accuracy: 0.9764\n",
      "Epoch 16, loss: 0.0689, accuracy: 0.9794\n",
      "Epoch 17, loss: 0.0680, accuracy: 0.9795\n",
      "Epoch 18, loss: 0.0694, accuracy: 0.9781\n",
      "Epoch 19, loss: 0.0656, accuracy: 0.9806\n",
      "Epoch 20, loss: 0.0663, accuracy: 0.9805\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "\n",
    "model = MnistCNN().to(dev)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "fit(20, model, loss_fun, optimizer, train_loader, val_loader, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0552, accuracy: 0.9832\n"
     ]
    }
   ],
   "source": [
    "test_dataset = MNIST(root='data/', train=False, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "test_loss, total, test_acc = evaluate(model, loss_fun, test_loader, metric=accuracy)\n",
    "print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to measur sparsity of a model\n",
    "def measure_module_sparsity(module, weight=True, bias=False, use_mask=False):\n",
    "    num_zeros = 0\n",
    "    num_elements = 0\n",
    "    if use_mask == True:\n",
    "        for buffer_name, buffer in module.named_buffers():\n",
    "            if \"weight_mask\" in buffer_name and weight == True:\n",
    "                num_zeros += torch.sum(buffer == 0).item()\n",
    "                num_elements += buffer.nelement()\n",
    "            if \"bias_mask\" in buffer_name and bias == True:\n",
    "                num_zeros += torch.sum(buffer == 0).item()\n",
    "                num_elements += buffer.nelement()\n",
    "    else:\n",
    "        for param_name, param in module.named_parameters():\n",
    "            if \"weight\" in param_name and weight == True:\n",
    "                num_zeros += torch.sum(param == 0).item()\n",
    "                num_elements += param.nelement()\n",
    "            if \"bias\" in param_name and bias == True:\n",
    "                num_zeros += torch.sum(param == 0).item()\n",
    "                num_elements += param.nelement()\n",
    "    sparsity = num_zeros / num_elements\n",
    "    return num_zeros, num_elements, sparsity\n",
    "\n",
    "def measure_global_sparsity(model, weight=True, bias=False, conv2d_use_mask=False, linear_use_mask=False):\n",
    "    num_zeros = 0\n",
    "    num_elements = 0\n",
    "    for module_name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(module, weight=weight, bias=bias, use_mask=conv2d_use_mask)\n",
    "            num_zeros += module_num_zeros\n",
    "            num_elements += module_num_elements\n",
    "\n",
    "        elif isinstance(module, torch.nn.Linear):\n",
    "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(module, weight=weight, bias=bias, use_mask=linear_use_mask)\n",
    "            num_zeros += module_num_zeros\n",
    "            num_elements += module_num_elements\n",
    "    sparsity = num_zeros / num_elements\n",
    "    return num_zeros, num_elements, sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRP_0(model : nn.Module, X):\n",
    "    layers = [module for module in model.named_children() if not isinstance(module, torch.nn.Sequential)]\n",
    "    L = len(layers)\n",
    "    A = [X] + [X] * L\n",
    "    # compute forward activations of NN\n",
    "    for layer in range(L):\n",
    "        if layers[layer][0] == 'pool1':\n",
    "            pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False, return_indices=True)\n",
    "            output1, indices1 = pool(A[layer])\n",
    "            A[layer + 1] = output1\n",
    "            continue\n",
    "        if layers[layer][0] == 'pool2':\n",
    "            pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False, return_indices=True)\n",
    "            output2, indices2 = pool(A[layer])\n",
    "            A[layer + 1] = output2\n",
    "            continue\n",
    "        A[layer + 1] = layers[layer][1].forward(A[layer])\n",
    "\n",
    "    T = A[-1].cpu().detach().numpy().tolist()[0]\n",
    "    index = T.index(max(T))\n",
    "    T = np.abs(np.array(T)) * 0\n",
    "    T[index] = 1\n",
    "    T = torch.FloatTensor(T)\n",
    "    R = [None] * L + [(A[-1].cpu() * T).data + 1e-6]\n",
    "    # do a backward to get relevance of neurons\n",
    "    # plase dont mind the if statements, I wasnt able to\n",
    "    # make it more generic\n",
    "    for layer in range(0, L)[::-1]:\n",
    "        if layers[layer][0] == 'flatten':\n",
    "            R[layer] = torch.reshape(A[layer], (32,7,7))\n",
    "            continue\n",
    "        if layers[layer][0] == 'pool2':\n",
    "            unpool = nn.MaxUnpool2d(kernel_size=2)\n",
    "            R[layer] = unpool(A[layer+1], indices2)\n",
    "            continue\n",
    "        if layers[layer][0] == 'pool1':\n",
    "            unpool = nn.MaxUnpool2d(kernel_size=2)\n",
    "            R[layer] = unpool(A[layer+1], indices1)\n",
    "            continue\n",
    "        if layers[layer][0] == 'relu1' or layers[layer][0] == 'relu2':\n",
    "            R[layer] = A[layer + 1]\n",
    "            continue\n",
    "        A[layer] = A[layer].data.requires_grad_(True)\n",
    "        z = newlayer(layers[layer][1]).forward(A[layer]) + 1e-9\n",
    "        s = (R[layer+1].to(dev) / z).data\n",
    "        (z * s).sum().backward()\n",
    "        c = A[layer].grad\n",
    "        R[layer] = (A[layer] * c).cpu().data \n",
    "    return R[0], R, L\n",
    "        \n",
    "# Create copy of a layer\n",
    "def newlayer(layer : nn.Module) -> nn.Module:\n",
    "    layer = copy.deepcopy(layer)\n",
    "    layer.weight = torch.nn.Parameter(layer.weight)\n",
    "    layer.bias = torch.nn.Parameter(layer.bias)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1beff6035e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Iterate throught the test data and collect relevance of neurons\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "image, label = next(iter(test_loader))\n",
    "model(image.to(dev))\n",
    "lrp_0, lrp, layers_len = LRP_0(model, image.to(dev))\n",
    "\n",
    "for image, label in test_loader:\n",
    "    out1, out2, _ = LRP_0(model, image.to(dev))\n",
    "    lrp_0 += out1\n",
    "    lrp += out2\n",
    "\n",
    "img = lrp_0.squeeze(0).permute(1,2,0).numpy()\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the relevance, prune the weights in the convs and linear layers\n",
    "# This is not very generic and I would like to spend some time after the exams\n",
    "# to make it as generic as possible\n",
    "# Please dont mind the l1_unstructered pruning, it just creates named buffer \n",
    "# for the weights mask, which gets then replaced by the mask computed from the LRP\n",
    "def prune_by_LRP(model : nn.Module, lrp, threshold_conv=10, threshold_fc_lower=-1e-5, threshold_fc_higher=1e-5):\n",
    "    model_copy = copy.deepcopy(model)\n",
    "    np_arr = lrp[1].cpu().data.numpy()\n",
    "    indexes = []\n",
    "    for x in range(np_arr.shape[1]):\n",
    "        relevance_sum = np.sum(np_arr[0][x])\n",
    "        if relevance_sum < threshold_conv:\n",
    "            indexes.append(x)\n",
    "    a = np.ones((model_copy.conv1.out_channels, model_copy.conv1.in_channels, model_copy.conv1.weight[0].shape[1], model_copy.conv1.weight[0].shape[2]))\n",
    "    a[indexes] = np.zeros((model_copy.conv1.in_channels, model_copy.conv1.weight[0].shape[1], model_copy.conv1.weight[0].shape[2]))\n",
    "    prune.l1_unstructured(model_copy.conv1, name='weight', amount=0.0)\n",
    "    with torch.no_grad():\n",
    "        prune.custom_from_mask(model_copy.conv1, name='weight', mask=torch.Tensor(a).to(dev))\n",
    "\n",
    "    np_arr = lrp[4].cpu().data.numpy()\n",
    "    indexes = []\n",
    "    for x in range(np_arr.shape[1]):\n",
    "        relevance_sum = np.sum(np_arr[0][x])\n",
    "        if relevance_sum < threshold_conv:\n",
    "            indexes.append(x)\n",
    "    a = np.ones((model_copy.conv2.out_channels, model_copy.conv2.in_channels, model_copy.conv2.weight[0].shape[1], model_copy.conv2.weight[0].shape[2]))\n",
    "    a[indexes] = np.zeros((model_copy.conv2.in_channels, model_copy.conv2.weight[0].shape[1], model_copy.conv2.weight[0].shape[2]))\n",
    "    prune.l1_unstructured(model_copy.conv2, name='weight', amount=0.0)\n",
    "    with torch.no_grad():\n",
    "        prune.custom_from_mask(model_copy.conv2, name='weight', mask=torch.Tensor(a).to(dev))\n",
    "\n",
    "    np_arr = lrp[-2].cpu().data.numpy()\n",
    "    indexes = []\n",
    "    for x in range(np_arr.shape[1]):\n",
    "        relevance_sum = np.sum(np_arr[0][x])\n",
    "        if relevance_sum > threshold_fc_lower and relevance_sum < threshold_fc_higher:\n",
    "            indexes.append(x)\n",
    "    prune.l1_unstructured(model_copy.out, name='weight', amount=0.0)\n",
    "    a = np.ones((list(model_copy.out.named_buffers())[0][1].shape[1]))\n",
    "    a[indexes] = 0\n",
    "    a = np.array([a] * 10)\n",
    "    with torch.no_grad():\n",
    "        prune.custom_from_mask(model_copy.out, name='weight', mask=torch.Tensor(a).to(dev))\n",
    "    return model_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of a pruned model: 0.1524\n",
      "Loss: 0.1682, accuracy: 0.9457\n",
      "Epoch 1, loss: 0.0786, accuracy: 0.9752\n",
      "Epoch 2, loss: 0.0788, accuracy: 0.9763\n",
      "Loss: 0.0675, accuracy: 0.9789\n"
     ]
    }
   ],
   "source": [
    "pruned_model = prune_by_LRP(model, lrp, 5)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "print(f\"Sparsity of a pruned model: {measure_global_sparsity(pruned_model, True, False, True, True)[-1]:.4f}\")\n",
    "test_loss, total, test_acc = evaluate(pruned_model, loss_fun, test_loader, metric=accuracy)\n",
    "print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Perform a finetuning of a pruned network\n",
    "optimizer = torch.optim.Adam(pruned_model.parameters(), lr=learning_rate)\n",
    "fit(2, pruned_model, loss_fun, optimizer, train_loader, val_loader, accuracy)\n",
    "\n",
    "test_loss, total, test_acc = evaluate(pruned_model, loss_fun, test_loader, metric=accuracy)\n",
    "print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of a pruned model: 0.3722\n",
      "Loss: 2.2596, accuracy: 0.5320\n",
      "Epoch 1, loss: 0.1313, accuracy: 0.9603\n",
      "Epoch 2, loss: 0.1107, accuracy: 0.9673\n",
      "Loss: 0.0974, accuracy: 0.9706\n"
     ]
    }
   ],
   "source": [
    "pruned_model = prune_by_LRP(model, lrp, 5, threshold_fc_lower=-1e-1, threshold_fc_higher=1e-1)\n",
    "print(f\"Sparsity of a pruned model: {measure_global_sparsity(pruned_model, True, False, True, True)[-1]:.4f}\")\n",
    "\n",
    "test_loss, total, test_acc = evaluate(pruned_model, loss_fun, test_loader, metric=accuracy)\n",
    "print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Perform a finetuning of a pruned network\n",
    "optimizer = torch.optim.Adam(pruned_model.parameters(), lr=learning_rate)\n",
    "fit(2, pruned_model, loss_fun, optimizer, train_loader, val_loader, accuracy)\n",
    "\n",
    "test_loss, total, test_acc = evaluate(pruned_model, loss_fun, test_loader, metric=accuracy)\n",
    "print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of a pruned model: 0.2355\n",
      "Loss: 0.2423, accuracy: 0.9184\n",
      "Epoch 1, loss: 0.0899, accuracy: 0.9721\n",
      "Epoch 2, loss: 0.0812, accuracy: 0.9757\n",
      "Loss: 0.0742, accuracy: 0.9778\n"
     ]
    }
   ],
   "source": [
    "pruned_model = prune_by_LRP(model, lrp, 10)\n",
    "print(f\"Sparsity of a pruned model: {measure_global_sparsity(pruned_model, True, False, True, True)[-1]:.4f}\")\n",
    "\n",
    "test_loss, total, test_acc = evaluate(pruned_model, loss_fun, test_loader, metric=accuracy)\n",
    "print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Perform a finetuning of a pruned network\n",
    "optimizer = torch.optim.Adam(pruned_model.parameters(), lr=learning_rate)\n",
    "fit(2, pruned_model, loss_fun, optimizer, train_loader, val_loader, accuracy)\n",
    "\n",
    "test_loss, total, test_acc = evaluate(pruned_model, loss_fun, test_loader, metric=accuracy)\n",
    "print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of a pruned model: 0.4553\n",
      "Loss: 2.3498, accuracy: 0.4673\n",
      "Epoch 1, loss: 0.1563, accuracy: 0.9543\n",
      "Epoch 2, loss: 0.1227, accuracy: 0.9637\n",
      "Loss: 0.1136, accuracy: 0.9672\n"
     ]
    }
   ],
   "source": [
    "pruned_model = prune_by_LRP(model, lrp, 10, threshold_fc_lower=-1e-1, threshold_fc_higher=1e-1)\n",
    "print(f\"Sparsity of a pruned model: {measure_global_sparsity(pruned_model, True, False, True, True)[-1]:.4f}\")\n",
    "\n",
    "test_loss, total, test_acc = evaluate(pruned_model, loss_fun, test_loader, metric=accuracy)\n",
    "print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Perform a finetuning of a pruned network\n",
    "optimizer = torch.optim.Adam(pruned_model.parameters(), lr=learning_rate)\n",
    "fit(2, pruned_model, loss_fun, optimizer, train_loader, val_loader, accuracy)\n",
    "\n",
    "test_loss, total, test_acc = evaluate(pruned_model, loss_fun, test_loader, metric=accuracy)\n",
    "print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of a pruned model: 0.2909\n",
      "Loss: 0.5474, accuracy: 0.8164\n",
      "Epoch 1, loss: 0.1097, accuracy: 0.9670\n",
      "Epoch 2, loss: 0.0998, accuracy: 0.9701\n",
      "Loss: 0.0905, accuracy: 0.9726\n"
     ]
    }
   ],
   "source": [
    "pruned_model = prune_by_LRP(model, lrp, 15)\n",
    "print(f\"Sparsity of a pruned model: {measure_global_sparsity(pruned_model, True, False, True, True)[-1]:.4f}\")\n",
    "\n",
    "test_loss, total, test_acc = evaluate(pruned_model, loss_fun, test_loader, metric=accuracy)\n",
    "print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Perform a finetuning of a pruned network\n",
    "optimizer = torch.optim.Adam(pruned_model.parameters(), lr=learning_rate)\n",
    "fit(2, pruned_model, loss_fun, optimizer, train_loader, val_loader, accuracy)\n",
    "\n",
    "test_loss, total, test_acc = evaluate(pruned_model, loss_fun, test_loader, metric=accuracy)\n",
    "print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of a pruned model: 0.5107\n",
      "Loss: 2.5187, accuracy: 0.4765\n",
      "Epoch 1, loss: 0.1750, accuracy: 0.9487\n",
      "Epoch 2, loss: 0.1394, accuracy: 0.9603\n",
      "Loss: 0.1287, accuracy: 0.9640\n"
     ]
    }
   ],
   "source": [
    "pruned_model = prune_by_LRP(model, lrp, 15, threshold_fc_lower=-1e-1, threshold_fc_higher=1e-1)\n",
    "print(f\"Sparsity of a pruned model: {measure_global_sparsity(pruned_model, True, False, True, True)[-1]:.4f}\")\n",
    "\n",
    "test_loss, total, test_acc = evaluate(pruned_model, loss_fun, test_loader, metric=accuracy)\n",
    "print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Perform a finetuning of a pruned network\n",
    "optimizer = torch.optim.Adam(pruned_model.parameters(), lr=learning_rate)\n",
    "fit(2, pruned_model, loss_fun, optimizer, train_loader, val_loader, accuracy)\n",
    "\n",
    "test_loss, total, test_acc = evaluate(pruned_model, loss_fun, test_loader, metric=accuracy)\n",
    "print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eda7e54fe21129b67f77862937907ee926f057597a3e2fa1e18ac955e40912b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
