{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from eval import accuracy\n",
    "from model import MnistCNN\n",
    "from train import fit, evaluate\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "NUM_CLASSES = 10\n",
    "INPUT_SIZE = 28 * 28\n",
    "DATASET = MNIST(root='data/', download=True, train=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\" \n",
    "\n",
    "def split_indices(n, val_pct):\n",
    "    # Determine size of validation set\n",
    "    n_val = int(val_pct*n)\n",
    "\n",
    "    idxs = np.random.permutation(n)\n",
    "\n",
    "    return idxs[n_val:], idxs[:n_val]\n",
    "\n",
    "train_indexes, validation_indexes = split_indices(len(DATASET), 0.2)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indexes)\n",
    "train_loader = DataLoader(DATASET, BATCH_SIZE, sampler=train_sampler)\n",
    "\n",
    "val_sampler = SubsetRandomSampler(validation_indexes)\n",
    "val_loader = DataLoader(DATASET, BATCH_SIZE, sampler=val_sampler)\n",
    "\n",
    "loss_fun = F.cross_entropy\n",
    "\n",
    "def predict_image(image, model):\n",
    "    xb = image.unsqueeze(0).to(dev)\n",
    "    yb = model(xb)\n",
    "    yb = yb.to(dev)\n",
    "    _, preds = torch.max(yb, dim=1)\n",
    "    return preds[0].item()\n",
    "\n",
    "def plot_graph(rng ,acc_list, lost_list, title):\n",
    "    plt.plot(rng, acc_list, '-b', label='accuracy')\n",
    "    plt.plot(rng, lost_list, '-r', label='loss')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlabel(\"amount pruned\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.0985, accuracy: 0.9705\n",
      "Epoch 2, loss: 0.0760, accuracy: 0.9758\n",
      "Epoch 3, loss: 0.0624, accuracy: 0.9818\n",
      "Epoch 4, loss: 0.0655, accuracy: 0.9808\n",
      "Epoch 5, loss: 0.0714, accuracy: 0.9798\n",
      "Epoch 6, loss: 0.0629, accuracy: 0.9835\n",
      "Epoch 7, loss: 0.0634, accuracy: 0.9832\n",
      "Epoch 8, loss: 0.0774, accuracy: 0.9803\n",
      "Epoch 9, loss: 0.0577, accuracy: 0.9845\n",
      "Epoch 10, loss: 0.0623, accuracy: 0.9842\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "model = MnistCNN().to(dev)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "fit(10, model, loss_fun, optimizer, train_loader, val_loader, accuracy)\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pt')\n",
    "# else:\n",
    "#     model = MnistCNN()\n",
    "#     model.load_state_dict(torch.load('model.pt', map_location=torch.device(dev)))\n",
    "#     model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0569, accuracy: 0.9846\n"
     ]
    }
   ],
   "source": [
    "test_dataset = MNIST(root='data/', train=False, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=200)\n",
    "test_loss, total, test_acc = evaluate(model, loss_fun, test_loader, metric=accuracy)\n",
    "print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_module_sparsity(module, weight=True, bias=False, use_mask=False):\n",
    "\n",
    "    num_zeros = 0\n",
    "    num_elements = 0\n",
    "\n",
    "    if use_mask == True:\n",
    "        for buffer_name, buffer in module.named_buffers():\n",
    "            if \"weight_mask\" in buffer_name and weight == True:\n",
    "                num_zeros += torch.sum(buffer == 0).item()\n",
    "                num_elements += buffer.nelement()\n",
    "            if \"bias_mask\" in buffer_name and bias == True:\n",
    "                num_zeros += torch.sum(buffer == 0).item()\n",
    "                num_elements += buffer.nelement()\n",
    "    else:\n",
    "        for param_name, param in module.named_parameters():\n",
    "            if \"weight\" in param_name and weight == True:\n",
    "                num_zeros += torch.sum(param == 0).item()\n",
    "                num_elements += param.nelement()\n",
    "            if \"bias\" in param_name and bias == True:\n",
    "                num_zeros += torch.sum(param == 0).item()\n",
    "                num_elements += param.nelement()\n",
    "\n",
    "    sparsity = num_zeros / num_elements\n",
    "\n",
    "    return num_zeros, num_elements, sparsity\n",
    "\n",
    "def measure_global_sparsity(model,\n",
    "                            weight=True,\n",
    "                            bias=False,\n",
    "                            conv2d_use_mask=False,\n",
    "                            linear_use_mask=False):\n",
    "\n",
    "    num_zeros = 0\n",
    "    num_elements = 0\n",
    "\n",
    "    for module_name, module in model.named_modules():\n",
    "\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "\n",
    "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
    "                module, weight=weight, bias=bias, use_mask=conv2d_use_mask)\n",
    "            num_zeros += module_num_zeros\n",
    "            num_elements += module_num_elements\n",
    "\n",
    "        elif isinstance(module, torch.nn.Linear):\n",
    "\n",
    "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
    "                module, weight=weight, bias=bias, use_mask=linear_use_mask)\n",
    "            num_zeros += module_num_zeros\n",
    "            num_elements += module_num_elements\n",
    "\n",
    "    sparsity = num_zeros / num_elements\n",
    "\n",
    "    return num_zeros, num_elements, sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount pruned: 0.1, Loss: 0.0571, accuracy: 0.9847\n",
      "sparsity: 0.09338642659279778.\n",
      "Epoch 1, loss: 0.0676, accuracy: 0.9836\n",
      "Epoch 2, loss: 0.0664, accuracy: 0.9857\n",
      "Epoch 3, loss: 0.0794, accuracy: 0.9827\n",
      "Epoch 4, loss: 0.0739, accuracy: 0.9844\n",
      "Epoch 5, loss: 0.0652, accuracy: 0.9861\n",
      "Loss: 0.0519, accuracy: 0.9863\n",
      "\n",
      "Amount pruned: 0.2, Loss: 0.0583, accuracy: 0.9840\n",
      "sparsity: 0.18708448753462603.\n",
      "Epoch 1, loss: 0.0621, accuracy: 0.9856\n",
      "Epoch 2, loss: 0.0651, accuracy: 0.9850\n",
      "Epoch 3, loss: 0.0655, accuracy: 0.9857\n",
      "Epoch 4, loss: 0.0651, accuracy: 0.9855\n",
      "Epoch 5, loss: 0.0731, accuracy: 0.9848\n",
      "Loss: 0.0550, accuracy: 0.9863\n",
      "\n",
      "Amount pruned: 0.30000000000000004, Loss: 0.0730, accuracy: 0.9784\n",
      "sparsity: 0.2789819944598338.\n",
      "Epoch 1, loss: 0.0614, accuracy: 0.9846\n",
      "Epoch 2, loss: 0.0712, accuracy: 0.9837\n",
      "Epoch 3, loss: 0.0693, accuracy: 0.9842\n",
      "Epoch 4, loss: 0.0723, accuracy: 0.9842\n",
      "Epoch 5, loss: 0.0697, accuracy: 0.9848\n",
      "Loss: 0.0538, accuracy: 0.9853\n",
      "\n",
      "Amount pruned: 0.4, Loss: 0.2171, accuracy: 0.9273\n",
      "sparsity: 0.3668282548476454.\n",
      "Epoch 1, loss: 0.0567, accuracy: 0.9843\n",
      "Epoch 2, loss: 0.0582, accuracy: 0.9841\n",
      "Epoch 3, loss: 0.0561, accuracy: 0.9856\n",
      "Epoch 4, loss: 0.0606, accuracy: 0.9849\n",
      "Epoch 5, loss: 0.0642, accuracy: 0.9843\n",
      "Loss: 0.0535, accuracy: 0.9850\n",
      "\n",
      "Amount pruned: 0.5, Loss: 2.2081, accuracy: 0.3100\n",
      "sparsity: 0.4391620498614958.\n",
      "Epoch 1, loss: 0.1386, accuracy: 0.9596\n",
      "Epoch 2, loss: 0.1085, accuracy: 0.9665\n",
      "Epoch 3, loss: 0.0934, accuracy: 0.9712\n",
      "Epoch 4, loss: 0.0880, accuracy: 0.9721\n",
      "Epoch 5, loss: 0.0843, accuracy: 0.9740\n",
      "Loss: 0.0749, accuracy: 0.9753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.1, 0.6, 0.1):\n",
    "    prune_model = MnistCNN()\n",
    "    prune_model.load_state_dict(copy.deepcopy(model.state_dict()))\n",
    "    optimizer = torch.optim.Adam(prune_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    parameters_to_prune = (\n",
    "        (prune_model.conv1, 'weight'),\n",
    "        (prune_model.conv2, 'weight'),\n",
    "        (prune_model.out, 'weight')\n",
    "    )\n",
    "\n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=i,\n",
    "    )\n",
    "\n",
    "    test_loss, total, test_acc = evaluate(prune_model.to(dev), loss_fun, test_loader, metric=accuracy)\n",
    "    print(f\"Amount pruned: {i}, Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")\n",
    "    num_zeros, num_elements, sparsity = measure_global_sparsity(prune_model, True, False, True, False)\n",
    "    print(f'sparsity: {sparsity}.')\n",
    "\n",
    "    # Perform finetuning of model\n",
    "    fit(5, prune_model, loss_fun, optimizer, train_loader, val_loader, accuracy) \n",
    "    test_loss, total, test_acc = evaluate(prune_model, loss_fun, test_loader, metric=accuracy)\n",
    "    print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount pruned: 0.1, Loss: 0.3008, accuracy: 0.9084\n",
      "sparsity: 0.046814404432132965.\n",
      "Epoch 1, loss: 0.0799, accuracy: 0.9775\n",
      "Epoch 2, loss: 0.0631, accuracy: 0.9827\n",
      "Epoch 3, loss: 0.0575, accuracy: 0.9857\n",
      "Epoch 4, loss: 0.0580, accuracy: 0.9846\n",
      "Epoch 5, loss: 0.0599, accuracy: 0.9848\n",
      "Loss: 0.0546, accuracy: 0.9856\n",
      "\n",
      "Amount pruned: 0.2, Loss: 1.0081, accuracy: 0.7278\n",
      "sparsity: 0.08999307479224376.\n",
      "Epoch 1, loss: 0.0647, accuracy: 0.9818\n",
      "Epoch 2, loss: 0.0629, accuracy: 0.9832\n",
      "Epoch 3, loss: 0.0565, accuracy: 0.9856\n",
      "Epoch 4, loss: 0.0580, accuracy: 0.9849\n",
      "Epoch 5, loss: 0.0579, accuracy: 0.9848\n",
      "Loss: 0.0455, accuracy: 0.9860\n",
      "\n",
      "Amount pruned: 0.30000000000000004, Loss: 0.7512, accuracy: 0.7570\n",
      "sparsity: 0.13639196675900278.\n",
      "Epoch 1, loss: 0.0688, accuracy: 0.9790\n",
      "Epoch 2, loss: 0.0564, accuracy: 0.9831\n",
      "Epoch 3, loss: 0.0630, accuracy: 0.9811\n",
      "Epoch 4, loss: 0.0549, accuracy: 0.9838\n",
      "Epoch 5, loss: 0.0577, accuracy: 0.9839\n",
      "Loss: 0.0483, accuracy: 0.9848\n",
      "\n",
      "Amount pruned: 0.4, Loss: 1.8453, accuracy: 0.4978\n",
      "sparsity: 0.1820983379501385.\n",
      "Epoch 1, loss: 0.0748, accuracy: 0.9781\n",
      "Epoch 2, loss: 0.0576, accuracy: 0.9828\n",
      "Epoch 3, loss: 0.0595, accuracy: 0.9821\n",
      "Epoch 4, loss: 0.0586, accuracy: 0.9833\n",
      "Epoch 5, loss: 0.0505, accuracy: 0.9851\n",
      "Loss: 0.0461, accuracy: 0.9853\n",
      "\n",
      "Amount pruned: 0.5, Loss: 3.2759, accuracy: 0.3065\n",
      "sparsity: 0.22901662049861496.\n",
      "Epoch 1, loss: 0.1102, accuracy: 0.9651\n",
      "Epoch 2, loss: 0.0822, accuracy: 0.9742\n",
      "Epoch 3, loss: 0.0665, accuracy: 0.9781\n",
      "Epoch 4, loss: 0.0677, accuracy: 0.9799\n",
      "Epoch 5, loss: 0.0605, accuracy: 0.9807\n",
      "Loss: 0.0579, accuracy: 0.9813\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.1, 0.6, 0.1):\n",
    "    prune_model = MnistCNN()\n",
    "    prune_model.load_state_dict(copy.deepcopy(model.state_dict()))\n",
    "    optimizer = torch.optim.Adam(prune_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    parameters_to_prune = (\n",
    "        (prune_model.conv1, 'weight'),\n",
    "        (prune_model.conv2, 'weight'),\n",
    "        (prune_model.out, 'weight')\n",
    "    )\n",
    "\n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.RandomUnstructured,\n",
    "        amount=i,\n",
    "    )\n",
    "\n",
    "    test_loss, total, test_acc = evaluate(prune_model.to(dev), loss_fun, test_loader, metric=accuracy)\n",
    "    print(f\"Amount pruned: {i}, Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")\n",
    "    num_zeros, num_elements, sparsity = measure_global_sparsity(prune_model, True, False, True, False)\n",
    "    print(f'sparsity: {sparsity}.')\n",
    "\n",
    "    # Perform finetuning of model\n",
    "    fit(5, prune_model, loss_fun, optimizer, train_loader, val_loader, accuracy) \n",
    "    test_loss, total, test_acc = evaluate(prune_model, loss_fun, test_loader, metric=accuracy)\n",
    "    print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ln_structured() got an unexpected keyword argument 'pruning_method'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m      4\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(prune_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[0;32m      6\u001b[0m parameters_to_prune \u001b[39m=\u001b[39m (\n\u001b[0;32m      7\u001b[0m     (prune_model\u001b[39m.\u001b[39mconv1, \u001b[39m'\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m      8\u001b[0m     (prune_model\u001b[39m.\u001b[39mconv2, \u001b[39m'\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m      9\u001b[0m     (prune_model\u001b[39m.\u001b[39mout, \u001b[39m'\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m prune\u001b[39m.\u001b[39;49mln_structured(\n\u001b[0;32m     13\u001b[0m     parameters_to_prune,\n\u001b[0;32m     14\u001b[0m     pruning_method\u001b[39m=\u001b[39;49mprune\u001b[39m.\u001b[39;49mL1Unstructured,\n\u001b[0;32m     15\u001b[0m     amount\u001b[39m=\u001b[39;49mi,\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m test_loss, total, test_acc \u001b[39m=\u001b[39m evaluate(prune_model\u001b[39m.\u001b[39mto(dev), loss_fun, test_loader, metric\u001b[39m=\u001b[39maccuracy)\n\u001b[0;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAmount pruned: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: ln_structured() got an unexpected keyword argument 'pruning_method'"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.1, 0.6, 0.1):\n",
    "    prune_model = MnistCNN()\n",
    "    prune_model.load_state_dict(copy.deepcopy(model.state_dict()))\n",
    "    optimizer = torch.optim.Adam(prune_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    parameters_to_prune = (\n",
    "        (prune_model.conv1, 'weight'),\n",
    "        (prune_model.conv2, 'weight'),\n",
    "        (prune_model.out, 'weight')\n",
    "    )\n",
    "\n",
    "    prune.ln_structured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=i,\n",
    "    )\n",
    "\n",
    "    test_loss, total, test_acc = evaluate(prune_model.to(dev), loss_fun, test_loader, metric=accuracy)\n",
    "    print(f\"Amount pruned: {i}, Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")\n",
    "    num_zeros, num_elements, sparsity = measure_global_sparsity(prune_model, True, False, True, False)\n",
    "    print(f'sparsity: {sparsity}.')\n",
    "\n",
    "    # Perform finetuning of model\n",
    "    fit(5, prune_model, loss_fun, optimizer, train_loader, val_loader, accuracy) \n",
    "    test_loss, total, test_acc = evaluate(prune_model, loss_fun, test_loader, metric=accuracy)\n",
    "    print(f\"Loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eda7e54fe21129b67f77862937907ee926f057597a3e2fa1e18ac955e40912b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
